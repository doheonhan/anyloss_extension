{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79042d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 1e-5\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "################################ MSE ################################\n",
    "def MSE(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.math.square(y_true-y_pred))\n",
    "\n",
    "################################ BCE ################################\n",
    "def BCE(y_true, y_pred):\n",
    "    return -tf.reduce_mean(y_true*tf.math.log(y_pred+s)+(1-y_true)*tf.math.log(1-y_pred+s))\n",
    "\n",
    "################################ WBCE ################################\n",
    "def WBCE(y_true, y_pred):\n",
    "    N = batch    # batch_size\n",
    "    y1 = tf.reduce_sum(y_true)\n",
    "    y0 = N-y1\n",
    "    w1 = y0/N #N/y1\n",
    "    w0 = y1/N #N/y0\n",
    "    return -tf.reduce_mean(w1*y_true*tf.math.log(y_pred+s)+w0*(1-y_true)*tf.math.log(1-y_pred+s))\n",
    "\n",
    "################################ TN/FP/FN/TP ################################\n",
    "def confusion_matrix(y_true, y_pred):\n",
    "    N = batch    # batch_size\n",
    "    y1 = tf.reduce_sum(y_true)\n",
    "    y0 = N-y1\n",
    "    TN = N-tf.reduce_sum(y_true)-tf.reduce_sum(y_pred)+tf.reduce_sum(y_true*y_pred)\n",
    "    FP = tf.reduce_sum(y_pred)-tf.reduce_sum(y_true*y_pred)\n",
    "    FN = tf.reduce_sum(y_true)-tf.reduce_sum(y_true*y_pred)\n",
    "    TP = tf.reduce_sum(y_true*y_pred)\n",
    "    return N, y1, y0, TN, FP, FN, TP\n",
    "\n",
    "################################ make_lists ################################\n",
    "def make_lists():\n",
    "    list_acc = []\n",
    "    list_f1 = []\n",
    "    list_gmean = []\n",
    "    list_bacc = []\n",
    "    list_pre = []\n",
    "    list_rec = []\n",
    "    list_spe = []\n",
    "    return list_acc, list_f1, list_gmean, list_bacc, list_pre, list_rec, list_spe\n",
    "    \n",
    "############################### Results ###############################\n",
    "def get_results(y_true, y_pred):\n",
    "    TN = metrics.confusion_matrix(y_true, y_pred)[0,0]\n",
    "    FP = metrics.confusion_matrix(y_true, y_pred)[0,1]\n",
    "    FN = metrics.confusion_matrix(y_true, y_pred)[1,0]\n",
    "    TP = metrics.confusion_matrix(y_true, y_pred)[1,1]\n",
    "    acc = np.round((TP+TN)/(TP+TN+FP+FN),4)\n",
    "    if TP+FP == 0:\n",
    "        pre = 0\n",
    "    else:\n",
    "        pre = np.round(TP/(TP+FP),4)\n",
    "    rec = np.round(TP/(TP+FN),4)\n",
    "    spe = np.round(TN/(TN+FP),4)\n",
    "    f1 = np.round(TP/(TP + 0.5*(FP+FN)),4)\n",
    "    f05 = np.round(TP/(TP + 0.8*FP + 0.2*FN),4)\n",
    "    f2 = np.round(TP/(TP + 0.2*FP + 0.8*FN),4)\n",
    "    gmean = np.round(((TP/(TP+FN)) * (TN/(TN+FP)))**0.5,4)\n",
    "    bacc = np.round(0.5*(TP/(TP+FN) + TN/(TN+FP)),4)\n",
    "    \n",
    "    list_acc.append(acc)\n",
    "    list_f1.append(f1)\n",
    "    list_gmean.append(gmean)\n",
    "    list_bacc.append(bacc)\n",
    "    list_pre.append(pre)\n",
    "    list_rec.append(rec)\n",
    "    list_spe.append(spe)\n",
    "\n",
    "################################ SPL ################################\n",
    "def splitter(y_pred):\n",
    "    return (0.5)**2-(y_pred-0.5)**2\n",
    "\n",
    "# =================================== Fbeta =================================== #\n",
    "################################ Pure_Fbeta ################################\n",
    "def Pure_Fbeta(y_true, y_pred):\n",
    "    b = 1 \n",
    "    N, y1, y0, TN, FP, FN, TP = confusion_matrix(y_true, y_pred)\n",
    "    F_beta = ((1+b**2)*TP) / ((b**2)*y1 + tf.reduce_sum(y_pred)+s)  # (1+b**2)TP/((1+b**2)TP+FP+b**2FN)\n",
    "    return 1-F_beta\n",
    "\n",
    "################################ Any_Fbeta ################################\n",
    "def Any_Fbeta(y_true, y_pred):\n",
    "    b = 1 \n",
    "    y_pred = 1/(1+tf.math.exp(-L*(y_pred-0.5)))\n",
    "    N, y1, y0, TN, FP, FN, TP = confusion_matrix(y_true, y_pred)\n",
    "    F_beta = ((1+b**2)*TP) / ((b**2)*y1 + tf.reduce_sum(y_pred)+s)  # (1+b**2)TP/((1+b**2)TP+FP+b**2FN)\n",
    "    return 1-F_beta\n",
    "\n",
    "################################ WBCEFL ################################\n",
    "def WBCEFL(y_true, y_pred):\n",
    "    b = 1\n",
    "    WBCEloss = WBCE(y_true, y_pred)\n",
    "    N, y1, y0, TN, FP, FN, TP = confusion_matrix(y_true, y_pred)\n",
    "    F_beta = ((1+b**2)*TP) / ((b**2)*y1 + tf.reduce_sum(y_pred)+s)  # (1+b**2)TP/((1+b**2)TP+FP+b**2FN)\n",
    "    return (1-r)*WBCEloss+(r)*(1-F_beta)\n",
    "\n",
    "################################ SPLFL ################################\n",
    "def SPLFL(y_true, y_pred):\n",
    "    b = 1\n",
    "    SPL = splitter(y_pred)\n",
    "    N, y1, y0, TN, FP, FN, TP = confusion_matrix(y_true, y_pred)\n",
    "    F_beta = ((1+b**2)*TP) / ((b**2)*y1 + tf.reduce_sum(y_pred)+s)  # (1+b**2)TP/((1+b**2)TP+FP+b**2FN)\n",
    "    return (1-w)*SPL+(w)*(1-F_beta)\n",
    "\n",
    "# =================================== Gmean =================================== #\n",
    "################################ Pure_Gmean ################################\n",
    "def Pure_Gmean(y_true, y_pred):\n",
    "    N, y1, y0, TN, FP, FN, TP = confusion_matrix(y_true, y_pred)\n",
    "    sur_gmean = (TP*TN)/(y1*y0+s)\n",
    "    return 1-sur_gmean\n",
    "\n",
    "################################ Any_Gmean ################################\n",
    "def Any_Gmean(y_true, y_pred):\n",
    "    y_pred = 1/(1+tf.math.exp(-L*(y_pred-0.5)))\n",
    "    N, y1, y0, TN, FP, FN, TP = confusion_matrix(y_true, y_pred)\n",
    "    sur_gmean = (TP*TN)/(y1*y0+s)\n",
    "    return 1-sur_gmean\n",
    "\n",
    "################################ WBCEGL ################################\n",
    "def WBCEGL(y_true, y_pred):\n",
    "    WBCEloss = WBCE(y_true, y_pred)\n",
    "    N, y1, y0, TN, FP, FN, TP = confusion_matrix(y_true, y_pred)\n",
    "    sur_gmean = (TP*TN)/(y1*y0+s)\n",
    "    return (1-r)*WBCEloss+(r)*(1-sur_gmean)\n",
    "\n",
    "################################ SPLGL ################################\n",
    "def SPLGL(y_true, y_pred):\n",
    "    SPL = splitter(y_pred)\n",
    "    N, y1, y0, TN, FP, FN, TP = confusion_matrix(y_true, y_pred)\n",
    "    sur_gmean = (TP*TN)/(y1*y0+s)\n",
    "    return (1-w)*SPL+(w)*(1-sur_gmean)\n",
    "\n",
    "# =================================== BAccu =================================== #\n",
    "################################ Pure_BAccu ################################\n",
    "def Pure_BAccu(y_true, y_pred):\n",
    "    N, y1, y0, TN, FP, FN, TP = confusion_matrix(y_true, y_pred)\n",
    "    baccu = (y0*TP+y1*TN) / (2*y1*y0+s)\n",
    "    return 1-baccu\n",
    "\n",
    "################################ Any_BAccu ################################\n",
    "def Any_BAccu(y_true, y_pred):\n",
    "    y_pred = 1/(1+tf.math.exp(-L*(y_pred-0.5)))\n",
    "    N, y1, y0, TN, FP, FN, TP = confusion_matrix(y_true, y_pred)\n",
    "    baccu = (y0*TP+y1*TN) / (2*y1*y0+s)\n",
    "    return 1-baccu\n",
    "\n",
    "################################ WBCEBL ################################\n",
    "def WBCEBL(y_true, y_pred):\n",
    "    WBCEloss = WBCE(y_true, y_pred)\n",
    "    N, y1, y0, TN, FP, FN, TP = confusion_matrix(y_true, y_pred)\n",
    "    baccu = (y0*TP+y1*TN) / (2*y1*y0+s)\n",
    "    return (1-r)*WBCEloss+(r)*(1-baccu)\n",
    "\n",
    "################################ SPLBL ################################\n",
    "def SPLBL(y_true, y_pred):\n",
    "    SPL = splitter(y_pred)\n",
    "    N, y1, y0, TN, FP, FN, TP = confusion_matrix(y_true, y_pred)\n",
    "    baccu = (y0*TP+y1*TN) / (2*y1*y0+s)\n",
    "    return (1-w)*SPL+(w)*(1-baccu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({'MSE':[0, 0, 0, 0, 0, 0, 0]}, index = ['Acc','F1','G_Mean','B_Acc','Pre','Rec','Spe'])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd16127",
   "metadata": {},
   "source": [
    "# Step1: Select One Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f309387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8f9bd17",
   "metadata": {},
   "source": [
    "## 1)SPAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311384ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203bd910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
    "data = pd.read_csv('spam.csv', encoding='latin1')\n",
    "print('sample number:',len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2192fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['Unnamed: 2']\n",
    "del data['Unnamed: 3']\n",
    "del data['Unnamed: 4']\n",
    "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fd125d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['v2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dbfd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "numlist = list(range(len(data)))\n",
    "data = data.set_index(pd.Index(numlist))\n",
    "data = data[:350]\n",
    "numlist = list(range(len(data)))\n",
    "data = data.set_index(pd.Index(numlist))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47683592",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[:175][data['v1'][:175] == 1].index, inplace=True)\n",
    "numlist = list(range(len(data)))\n",
    "data = data.set_index(pd.Index(numlist))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81909ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['v2']\n",
    "y = data['v1']\n",
    "y = y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf4882",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)\n",
    "\n",
    "batch = 128\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57feca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c0c3dd3",
   "metadata": {},
   "source": [
    "## 2)Reuters News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02871f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/thedevastator/uncovering-financial-insights-with-the-reuters-2?select=ModApte_test.csv\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ModHayes_train.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae57c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"text\",\"topics\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topics'].value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de55c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "isblank = data['topics'] == \"[]\"\n",
    "isacq = data['topics'] == \"['acq']\"\n",
    "data = data[isblank | isacq]\n",
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topics'] = data['topics'].replace([\"[]\",\"['acq']\"],[0,1])\n",
    "numlist = list(range(len(data)))\n",
    "data = data.set_index(pd.Index(numlist))\n",
    "data = data[:350]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fb9cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topics'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aabab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[:200][data['topics'][:200] == 1].index, inplace=True)\n",
    "numlist = list(range(len(data)))\n",
    "data = data.set_index(pd.Index(numlist))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['topics'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea09077",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['text']\n",
    "y = data['topics']\n",
    "y = y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b96b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)\n",
    "\n",
    "batch = 64\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96bb2f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89f12894",
   "metadata": {},
   "source": [
    "## 3)IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a13bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ad07ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9707bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'] = data['sentiment'].replace(['negative','positive'],[0,1])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c9ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data['review'][:400]\n",
    "y_train = data['sentiment'][:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b325c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making lists of index\n",
    "idx_0 = []\n",
    "idx_1 = []\n",
    "for i in range(len(y_train)):\n",
    "    if list(y_train)[i] == 0:\n",
    "        idx_0.append(i)\n",
    "    if list(y_train)[i] == 1:\n",
    "        idx_1.append(i)\n",
    "print(len(idx_0), len(idx_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba44cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1_new = idx_1[:40]\n",
    "\n",
    "idx = idx_0 + idx_1_new\n",
    "idx.sort()\n",
    "print(len(idx), idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11098e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in idx:\n",
    "    X.append(X_train[i])\n",
    "    y.append(y_train[i])\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = y.astype(float)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc142c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 2)\n",
    "\n",
    "batch = 128\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42274d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e170fc3",
   "metadata": {},
   "source": [
    "# Step2: Do BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb918f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "steps_per_epoch = int(len(X)*0.4)  # length of train data\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "tfhub_handle_encoder ='https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n",
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "#     net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')(net)     # activation=None\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfff03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_acc = []\n",
    "mse_f1 = []\n",
    "mse_gmean = []\n",
    "mse_bacc = []\n",
    "mse_pre = []\n",
    "mse_rec = []\n",
    "mse_spe = []\n",
    "\n",
    "for i in range(5):\n",
    "    print('#'*50,'{0}th repeat'.format(i+1),'#'*50)\n",
    "    list_acc, list_f1, list_gmean, list_bacc, list_pre, list_rec, list_spe = make_lists()\n",
    "\n",
    "    n_iter=0\n",
    "    ###################### MLP (sigmoid // MSE) ##############################\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        n_iter += 1\n",
    "        X_train = X[train_index]\n",
    "        y_train= y[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_test= y[test_index]\n",
    "#         print('#'*50,'{0}th CV'.format(n_iter),'#'*50)    \n",
    "        X_train = tf.convert_to_tensor(X_train, dtype=tf.string, dtype_hint=None, name=None)\n",
    "        X_test = tf.convert_to_tensor(X_test, dtype=tf.string, dtype_hint=None, name=None)\n",
    "        y_train = tf.convert_to_tensor(y_train, dtype=tf.float32, dtype_hint=None, name=None)\n",
    "        y_test = tf.convert_to_tensor(y_test, dtype=tf.float32, dtype_hint=None, name=None)\n",
    "\n",
    "        model = build_classifier_model()\n",
    "        early_stopping = EarlyStopping(monitor='loss', patience=0, min_delta=1e-4)\n",
    "    #     check_point = ModelCheckpoint('best_model.h5', monitor=\"val_loss\", save_best_only=True, save_weights_only=True)\n",
    "        opt = optimization.create_optimizer(init_lr=0.0002,num_train_steps=num_train_steps,num_warmup_steps=num_warmup_steps,optimizer_type='adamw')\n",
    "        model.compile(optimizer=opt, loss=MSE, metrics=['Accuracy'])\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_test, y_test), verbose=0, epochs=epochs, batch_size = batch, ) \n",
    "                            #callbacks=[early_stopping]) # , check_point])\n",
    "#         plt.plot(history.history['loss'], label='loss')\n",
    "#         plt.ylim([0, 1])\n",
    "#         plt.xlabel('Iteration',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.ylabel('Loss',fontweight=\"bold\",fontsize = 15)\n",
    "#         plt.title(\"Cost Function\",fontweight=\"bold\",fontsize = 20)\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        predicted = []\n",
    "    #     model.load_weights('best_model.h5')  # Best Model by Check_Point\n",
    "        result = model.predict(X_test)\n",
    "        for i in range(X_test.shape[0]):\n",
    "            if result[i] <= 0.5:\n",
    "                predicted.append(0)\n",
    "            else:\n",
    "                predicted.append(1)\n",
    "        get_results(y_test, predicted)\n",
    "    print(\"Acc:{}\\nF1:{}\\nGM:{}\\nBA:{}\\nPRE:{}\\nREC:{}\\nSPE:{}\\n\".format(np.mean(list_acc),np.mean(list_f1),np.mean(list_gmean),\n",
    "                                                                         np.mean(list_bacc),np.mean(list_pre),np.mean(list_rec),\n",
    "                                                                         np.mean(list_spe)))     \n",
    "    mse_acc.append(np.mean(list_acc))\n",
    "    mse_f1.append(np.mean(list_f1))\n",
    "    mse_gmean.append(np.mean(list_gmean))\n",
    "    mse_bacc.append(np.mean(list_bacc))\n",
    "    mse_pre.append(np.mean(list_pre))\n",
    "    mse_rec.append(np.mean(list_rec))\n",
    "    mse_spe.append(np.mean(list_spe))\n",
    "               \n",
    "res['MSE'] = [np.mean(mse_acc), np.mean(mse_f1), np.mean(mse_gmean), np.mean(mse_bacc), \n",
    "              np.mean(mse_pre), np.mean(mse_rec), np.mean(mse_spe)]\n",
    "res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AC:\", np.round(np.mean(mse_acc),4),'±',np.round(np.std(mse_acc),4))\n",
    "print(\"F1:\", np.round(np.mean(mse_f1),4),'±',np.round(np.std(mse_f1),4))\n",
    "print(\"GM:\", np.round(np.mean(mse_gmean),4),'±',np.round(np.std(mse_gmean),4))\n",
    "print(\"BA:\", np.round(np.mean(mse_bacc),4),'±',np.round(np.std(mse_bacc),4))\n",
    "print(\"PRE:\", np.round(np.mean(mse_pre),4),'±',np.round(np.std(mse_pre),4))\n",
    "print(\"REC:\", np.round(np.mean(mse_rec),4),'±',np.round(np.std(mse_rec),4))\n",
    "print(\"SPE:\", np.round(np.mean(mse_spe),4),'±',np.round(np.std(mse_spe),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a517885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each loss, do above\n",
    "# LOSS = [MSE, BCE, WBCE, Pure_Fbeta, Any_Fbeta, WBCEFL, SPLFL, \n",
    "#         Pure_Gmean, Any_Gmean, WBCEGL, SPLGL, \n",
    "#         Pure_BAccu, Any_BAccu, WBCEBL, SPLBL ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55283213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Results\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c168a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768cc2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352261e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f1b793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
